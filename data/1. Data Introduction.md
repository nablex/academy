# Data Models

In the 21st century, data is the most valuable commodity most companies have. However, data is only as valuable the meaning you can derive from it.
More specifically if you have 50GB of random bits, you might have a lot of data, but not a lot of use for it, it is therefor worthless.

If you have 50GB of structured data however, you can start drawing conclusions, build new applications that extend upon that data, expose it to clients for self-service, combine it with other data sets to create entirely new business models...

It is therefor very important to structure the data correctly so it can have maximum value to the customer, both in the short term _and_ the long term.

## Modeling

When we talk about data modeling, we talk bout defining the structure of the data, more specifically which data we persist and how it relates to other data. There are many other important questions regarding that data (where does it come from, where does it go...) but the data models involved in those questions are generally derivations from that underlying persisted data, your core model.

When we design a new application, getting that core data model right is the most important step of the process. First you need to gather all the requirements, you need to know which business cases you need to support now and preferably which you will likely need to support in the future. Add to that a good measure of common sense and experience and you have a scaleable model.

**Important**: changing your application down the line (due to new insights, new features,...) is generally easy. Changing your data model is harder, if for example you have 2 years worth of data at that point, you can't retroactively "invent" missing data if you forgot to capture something. Transforming your data into a new model is also a lot harder than changing the application around it.

When you are unsure if you need something, we tend to err on the side of "overcapture" rather than "undercapture". Too much data can become problematic at high volumes, but too little data is even worse. A notable exception to this is [GDPR](https://en.wikipedia.org/wiki/General_Data_Protection_Regulation) which requires you to follow specific rules when it comes to [personally identifiable](https://en.wikipedia.org/wiki/Personal_data) data.

Nabu can help you nagivate the GDPR rules but this will be addressed in a different article.

## Model Types

In modeling we make a distinction between "functional" modeling and "technical" modeling. Functional modeling is generally the first step where you capture the requirements in a data model that can completely represent your particular problem without caring about technical details.

The technical model starts from the functional model but takes into account certain technical realities. For example you might reuse existing concepts, merge similar concepts etc. A good example of this is the CMS (content management system) that comes in Nabu: it has a ton of core tables which we usually extend and that have a lot of utilities around them. If your functional model for example required attachments, you might leave it out completely in your technical model knowing that the CMS can handle this out of the box.

Given enough experience and a small enough project, you can skip the functional model and go straight to the technical model.

It is important to keep your technical model well documented, especially when the transition from functional to technical is not immediately obvious.

## Model Tools

There are a lot of ways you can model data, not only are there a lot of tools for each format, there are quite a few formats as well:

- [XML Schema](https://en.wikipedia.org/wiki/XML_schema): this really took off in the age of SOAP/WSDL and is still used a lot in corporate settings
- [UML](https://en.wikipedia.org/wiki/Unified_Modeling_Language): can model more than just data
- [JSON Schema](https://en.wikipedia.org/wiki/JSON#JSON_Schema)
- Java Beans: some people prefer to simply write code

Nabu has a generic `types-api` layer that abstracts the actual design language used and supports all of the above and more. This means if you have XML schema from a client, you can give that to Nabu and it will work, if you have existing java beans from a java library, that will work equally well.

Nabu also comes with `structures` which is the [reference implementation](https://en.wikipedia.org/wiki/Reference_implementation) of the `types-api` API. Structures can do pretty much everything you can do in other modeling languages and have some additional unique features that make it a powerful tool.

For a long time we used UML to model our data, but we are shifting to structure-based modeling for new applications.

# Data Validity

When it comes to how we store and treat data there are two distinctive approaches:

- :dynamic styping/nosql/...: we don't spend a lot of time specifically defining, modeling and enforcing data validity, instead dynamically try to do our best with what we have at runtime
- :static typing/relational databases/...: we spend much more time defining and modeling everything beforehand, knowing that this provides us with runtime guarantees about how the application will behave

To give you an example of the dynamic typing approach, suppose you have a query that performs ``a+b``. What will this do at runtime? You might instinctively say: well it sums the two values into an end result. 

The reality however is very much dependent on what ``a`` and ``b`` are. If both are numbers, for example ``a=1`` and ``b=2``, you will get the end result you would think, being the mathematical summation into ``3``. However, if you never mandate that they are numeric, at runtime you might get all sorts of values, a few examples:

[:.resources/possibilities.png]

There are of course many, _many_ more combinations. To make matters even worse, the results you see here are specific to this environment. Running it in different languages or environments can yield different results.

If you look at only that part of the application, and want to predict what will _actually_ happen, you have to see who is running this particular query and what their specific values of ``a`` and ``b`` are. Suppose however that one of the parameters is an input or an endresult of yet another unpredictable operation and you will have to check even further down the line who is calling _that_ query. This is a very simple example on a very small scale. Given a large enough application it becomes impossible to predict what will actually happen. You have to run it and hope for the best. 

The only thing we need to do to make this more stable, is to explicitly mandate that a and b must be numbers. Once we do that, the query becomes predictable and it **no longer matters who is running it**, the query will always do the same. This allows you to look at only a very specific part of a large application and understand exactly what that tiny bit will be doing at runtime.

This is what we call **design-time guarantees**, because while we are building the application, we get guarantees about how the application will behave at runtime by **imposing restrictions** and **being specific** about the kind of data we are operating on. 

Because most things you do in a business application will revolve around the core data model in one way or the other, that model will directly or indirectly dictate how large parts of the application are modelled, this is why it becomes even more important to get this right.

An important additional advantage from explicitly mandating types is that in case of unexpected input, the application fails immediately.

Suppose in the above example someone sends in data other than numbers, you want your application to fail then and there, at the earliest opportunity to catch this invalid data. If instead your query returns an attempt at an answer, which is equally unexpected as the inputs, your application will keep chugging along, passing that invalid answer to another piece of code and to another right up until it either actually fails or -worse- until the invalid data is persisted. 

However, when your application fails 10 steps after that original query, it becomes _very_ hard to figure out _why_ it failed. Deducing retroactively that the culprite was an invalid value for `a` can quickly become impossible. And if you don't understand why or how your application is failing, it becomes really hard to fix it. The rule of thumb is that we want to **fail as early as possible** in case something is wrong.

## Specificity

As we saw in the above example, the most important part of designing your structured data is to be _specific_. Not only should you think about which data types might work, you need to think about the most specific thing you can say about that data.

If you have an "amount" field indicating the amount of items in an order. Can it (technically) be a string? Sure that can work. All numbers can be displayed as string. However, not all valid strings are valid numbers, so defining it as a number is more specific and therefor the better option.

Do you know for a fact it can not have a decimal part? Make it an integer instead of a double. 
Do you know it can never be negative? Set the minInclusive to 0. 
Are you sure it is always filled in? Make it mandatory. 
And so on and so forth. 

The more specific you can be, the better. Not only does this make your application even more predictable (and thus less buggy), it increases readability for those who have to look at the application or its data in the future (including your future self).

An important rule here is: you can always make your data field "less specific" in the future in case you want to support new usecases because the accumulated data so far will definitely match the less strict rule. You can almost never make it "more specific" however as the accumulated data up to that point is likely to conflict with the more stringent requirements.

Apart from restrictions on a single field, you can also add guarantees so that your data as a whole is coherent. For example if your order is linked to a company, you can ensure that the company actually exists at all times, making it impossible to "accidently" delete and hence invalidate the usefullness of the order data.

The specificity also encompasses defining each and every field. If you need to add new fields to a running application, you need to carefully think about the transition path of the accumulated data and the business logic, not simply add it, wrap your business logic in a big "if this new field exists" and walk away. This will lead to very fragile solutions and unreliable datasets that are harder to build upon in the future.

## Validation

The specificity you define is only useful if you enforce it. Adding a comment stating that `a` and `b` should really be numbers but not enforcing this at any point, makes it rather useless as the application might still behave differently.

Nabu supports all the validations available in [XML Schema](https://en.wikipedia.org/wiki/XML_schema) and some other specifications like java beans validation. Without listing them all here, this includes things like patterns and length requirements on strings, range requirements on numbers and much more.

One interesting additional level of validation is rule-based validation where you create validation rules that span across multiple elements or require more complex logic, for example: a is optional _if_ b has a particular value. This is usually handled at the application level though for the curious reader, I would suggest checking out [schematron](https://en.wikipedia.org/wiki/Schematron).

### Backend

In the Nabu backend, basic things like the data type (number, string...) are enforced by default (unless you use the string property `actualType`). Further restrictions like patterns, length requirements, are enforced if you enable the input/output validation toggles or specifically call a validate routine.

The reason Nabu allows you to explicitly allow invalid data, is for cases where you want custom handling of that invalid data. Perhaps you want to create a human task for someone else to have a look at it and correct it rather than throwing a hard validation error to whoever is calling the application. Sometimes capturing invalid data is better than capturing no data at all.

The rule-of-thumb is: whenever data is entering or exiting the Nabu server, you should enable the input/output validation unless you have a good reason not to.

### Frontend

As a rule: the backend should _never_ trust data coming from an enduser. Any enduser can be a bad actor trying to manipulate the system in ways it was not designed for. This requires security at multiple levels but also data validation of anything that enters the system.

This means the backend _must_ perform these validations. However, from a UX perspective it is not ideal that a valid user has to wait until he has filled in an entire form _and_ submitted it to the backend before being alerted to potential problems.

To that end, you usually also want to perform the same validation checks in the frontend. It is of course important that the validations in the frontend match those in the backend. If the frontend is unnecessarily strict, things are flagged that are actually allowed, or the other way around, it could be that the frontend indicates to the user that everything is OK, only to receive an error from the backend.

Nabu solves this automatically. How it does this is that the data restrictions that are enforced in the backend are also declared in the swagger it generates. That swagger is used by the frontend and understood by the form components who automatically check those same validation rules. Page builder makes sure all the form components are correctly configured, transparently completing the circle enabling easy frontend validation.

## Storage Guarantees

Relational databases don't always allow for the level of finegrained validation we have in Nabu, however they do allow validating quite a bit. The same rules apply here: the more strict you can be, the better as it provides more guarantees. But it is equally important that this is in sync with the application.

Nabu can help you to keep these two in sync by adding data structures to the automatically managed type list. At that point Nabu will perform the necessary checks to make sure the two are in sync.

The database does have some additional validation capabilities that are not easily enforced at the application level without querying the database. For example unique constraints guarantee that a certain value or combination of values is unique across the dataset. Only the database can truly enforce this. The application can do this manually by performing selects and checking this but it is easier and generally the better option to let the database enforce this. 

By setting all these restrictions at the database level, it guarantees that all the data truly adheres to this rule, not only the data that has passed through the proper application.

One of the most important guarantees a relational database can offer is in the form of foreign keys. It guarantees that when you are referencing some other piece of data, that that piece of data actually exists.

# Data Consistency 

## Single Truth

An important part of designing your data structures is to ensure that each particular piece of information is only stored once. Suppose you have the company name present in two fields, and at some point you update one of those two fields. You now have two fields contradicting each other when it comes to the name of the company. Which one is correct?

Duplication is in 99% of the cases a bad idea, but once every so often it _is_ the best solution given certain performance challenges. However, it must be a very well considered, well documented decision, not a key design philosophy.

The same goes for derivative data, if you have an order and it has orderlines. Each orderline has (presumably) a "unitPrice" and an "amount" field. Calculating the total value of an orderline is then simply ``unitPrice * amount``. Do you store this in a third column "total" or do you calculate it on the fly? Same goes for the order: the total of the order is (simplistically) the sum of all the orderline totals.

Do you store it separately and thus create a duplicate that needs to be kept in sync, or do you calculate it at runtime?

The answer to this is less black-and-white and depends on the circumstances, the end result is however indeniably linked to the separate fields, updating the core values without updating the end result will end in a similar inconsistency.

There are alternatives like database "views". These provide different views on existing tables and (by default) are calculated on the fly. In case the calculations are too complex, you can opt for ``materialized views`` which store the end result until you force it to recalculate. You still risk accessing stale data, but at least updating the data to the latest state is trivial (recompute the view). In the case of the orderlines you need to know the business logic behind the data to recalculate it correctly.

One feature being considered for Nabu is computed fields that would act like regular fields but are actually derivatives of other fields.

## Applicative Mutation

Mutating data often needs to follow a set of business rules. For example perhaps an order can only be added to a company _if_ said company has been verified by your legal department. Employees can only be removed from the database if HR confirms that he was actually fired etc.

This mutation logic is best captured in the application logic. Ideally however, this is a separate layer within the application, as close to the database as possible, idealogically you should think of this as a microservice within your application, separating it from higher level parts.

## Data Integrity

When you are modifying complex datasets, you want an "all or nothing" approach. If you need to create an order and some orderlines, you want to be sure that both the order and the lines exist in case of success and in case of some failure that neither exists. You don't want to end up with a partial state where for example the order was persisted but the orderlines were not because correctly recovering from such an intermediate state is non-trivial.

To this end you can use transactions. In nabu transactions are automatically handled which means you generally don't have to care about it. If those few cases that it _does_ matter you can take control over the transactions in a number of ways, though in my experience this is mostly when you are creating frameworks rather than business solutions.

For the interested reader, I suggest reading up on [ACID](https://en.wikipedia.org/wiki/ACID), a set of properties of database transactions that deal with the guarantees a database offers.

A transaction generally has a set of statements that are "staged", followed by a final "commit" or "rollback" depending on the success or failure of the end result. There can be multiple statements, but they are limited to a single system (e.g. a single database). If you want to perform transactional operations cross system (e.g. from one database to another), there are [two-phase commit transactions](https://en.wikipedia.org/wiki/Two-phase_commit_protocol), this is off topic for this article however.

Note that transactions lead to locking which can have different impacts depending on the [transaction isolation|https://en.wikipedia.org/wiki/Isolation_(database_systems)]. Again, the details of this are outside the scope of this document but interested readers can dig deeper by checking out the link. 

# Standard fields

There are some fields that will be present in a lot of tables, regardless of the data they contain.

## Primary Keys

The most important of this is the primary key: as a general rule of thumb every record should be identifiable by a single unique field. It is technically possible to have "complex" primary keys where the combination of multiple fields is defined as the primary key but I strongly advise against this. If you have such a combination of fields, add a unique constraint...and a separate new primary key field.

This should preferably be an internal id, not an external one. For example a VAT can be considered to be an external id of a company. External ids can get special treatment (e.g. unique restrictions etc) but should not be your primary key.

In general there are two accepted types of primary keys:

- :sequence: a number that increases every time you create a new record. Usually there is a single sequence per table meaning the value is locally unique within that table.
- :uuid: a [universal unique identifier](https://en.wikipedia.org/wiki/Universally_unique_identifier). There are 4 variants, some of which are less than ideal to use (as they leak internal information), we generally use type 4 securely randomly generated ids.

The added advantage of a uuid is that it is unique _cross_ table and even cross database whereas numeric keys are only unique for a given table.
The downside of uuids is that they take up more space and are difficult to "quickly" reference in a conversation. "Hey, check out order 345" is a lot easier than "Hey, check out order 123e4567-e89b-12d3-a456-426652340000".

Another upside of uuids is that they can be generated without going to the database. This means, at the application level you can generate your id, use it in various contexts, and asynchronously persist it in the database at a later time. Sequences are managed and generated by the database so you need a roundtrip to the database to get the next id.

The downside of uuids is that they are not sequential, there is no inherent order to uuids. Sequences are guaranteed to be in order of creation (more or less). Note that sequences are guaranteed to be unique and counting upwards, they are _not_ guaranteed to be sequential!

We almost always use UUIDs instead of sequences.

### Sequences

If you want to use a sequence in nabu, you can select your numeric field and set the property `generated` to true. At that point, nabu will no longer send that field (by default) when doing an insert, letting the database fill it in.

Additionally nabu will generate a sequence declaration in the DDL, for example:

```
create sequence seq_pharmacies_id;
create table pharmacies (
	id integer primary key default nextval('seq_pharmacies_id'),
	licensee text not null
);
```

In the SQL services you can fill in the "generated column name" property at the top, at that point the actual number that was generated will appear in the output in the `generatedKeys` section.

## Metadata

There are two columns we often add to our tables:

- :created: a datetime indicating when the record was initially created
- :modified: a datetime indicating when the record was last modified

These fields are rarely used by the application itself but act as minimal metadata for records, allowing for some quick insights for those looking at said data. There are other systems (auditing, historization,...) should you want more structural information about the data and how it evolved over time.

## Naming

Most database are _not_ case sensitive. Nabu however follows the lower camelcase naming convention for all of its naming, including that of fields. We prefer to keep the application consistent in that regard, even when the database might not support it.

Nabu will automatically transform camelcase to underscore based and the other way around when communicating with databases. Should you have a usecase where the database does not have an underscore-based naming policy, for example "organisationid" is written without an underscore, you can model this in nabu by avoiding camelcase and naming it "organisationid" in lowercase. The ``alias`` property also allows for modeling names that deviate from how we want to call them internally or because of naming policies are simply not allowed in Nabu.

There are a few other things to keep in mind when naming fields or tables:

- there are a lot of reserved words that can not be used without special handling, we preferably just avoid those
- some databases (like oracle) have a limit on how long the column and table names can be

Some additional things to look out for:

- short words are better than long ones, avoid repetition or unnecessary additions, for example "vat" might be better than "vatIdentifier" which is in turn better than "organisationVatIdentifier", especially when it resides in the table "organisations" at which point the first part is a repetition of its context.
- make sure the name is correct and not misleading, calling a field "amount" and storing the price might not be what people expect.
- only use abbreviations that are widely understood
- be consistent, don't call the first field "startDate" and the second "stopped" if both are dateTimes.

Some conventions that we have adopted:

- we use "plural" for naming tables, specifically to avoid conflicts with reserved words, for example we will call a table "organisations" rather than "organisation".
- we generally use a verb to denote dateTimes, for example "published" or "started" rather than "publishDate" or "startDate". The latter is a form of [hungarian notation](https://en.wikipedia.org/wiki/Hungarian_notation) and counts as repetition. in a lot of cases a datetime is a boolean value with more information. For example if an article is "published" (type boolean), it simply denotes yes or no. If an article is published (dateTime), it also denotes yes or no, but compared to a specific point in time.
- we always call the primary key field "id", foreign keys to that field are generally the name of the singular object + "Id". for example if you were to create a foreign key to the organisations table, it will often be called "organisationId". There are good reasons to deviate from this from time to time however.
